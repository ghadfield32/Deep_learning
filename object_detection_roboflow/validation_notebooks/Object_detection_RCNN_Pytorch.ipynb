{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qj526THT0vCv",
        "outputId": "9d01a049-14f1-4076-99f8-b0bd25d0f361"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Dependency ultralytics==8.0.196 is required but found version=8.0.178, to fix: `pip install ultralytics==8.0.196`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading Dataset Version Zip in Basketball-2 to yolov8::  61%|██████▏   | 171852/279757 [00:20<00:12, 8349.97it/s] \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ghadf\\OneDrive\\Desktop\\Data Analytics\\Python\\DL\\git\\Deep_learning\\object_detection_roboflow\\validation_notebooks\\Object_detection_RCNN_Pytorch.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m rf \u001b[39m=\u001b[39m Roboflow(api_key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhtpcxp3XQh7SsgMfjJns\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m project \u001b[39m=\u001b[39m rf\u001b[39m.\u001b[39mworkspace(\u001b[39m\"\u001b[39m\u001b[39mownprojects\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mproject(\u001b[39m\"\u001b[39m\u001b[39mbasketball-w2xcw\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m dataset \u001b[39m=\u001b[39m project\u001b[39m.\u001b[39;49mversion(\u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49mdownload(\u001b[39m\"\u001b[39;49m\u001b[39myolov8\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\ghadf\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\roboflow\\core\\version.py:231\u001b[0m, in \u001b[0;36mVersion.download\u001b[1;34m(self, model_format, location, overwrite)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[39mexcept\u001b[39;00m json\u001b[39m.\u001b[39mJSONDecodeError:\n\u001b[0;32m    229\u001b[0m             response\u001b[39m.\u001b[39mraise_for_status()\n\u001b[1;32m--> 231\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__download_zip(link, location, model_format)\n\u001b[0;32m    232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__extract_zip(location, model_format)\n\u001b[0;32m    233\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reformat_yaml(location, model_format)\n",
            "File \u001b[1;32mc:\\Users\\ghadf\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\roboflow\\core\\version.py:666\u001b[0m, in \u001b[0;36mVersion.__download_zip\u001b[1;34m(self, link, location, format)\u001b[0m\n\u001b[0;32m    664\u001b[0m             \u001b[39mif\u001b[39;00m chunk:\n\u001b[0;32m    665\u001b[0m                 f\u001b[39m.\u001b[39mwrite(chunk)\n\u001b[1;32m--> 666\u001b[0m                 f\u001b[39m.\u001b[39mflush()\n\u001b[0;32m    668\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    669\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError when trying to download dataset @ \u001b[39m\u001b[39m{\u001b[39;00mlink\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"htpcxp3XQh7SsgMfjJns\")\n",
        "project = rf.workspace(\"ownprojects\").project(\"basketball-w2xcw\")\n",
        "dataset = project.version(2).download(\"yolov8\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XuNHBBTU0281",
        "outputId": "649229dc-299c-4c9d-c545-59b5db09d69f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.0.1+cpu'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Note: this notebook requires torch >= 1.10.0\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "swdAsRfj079K",
        "outputId": "94471175-b40a-4457-e484-ebfa808b97e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Setup device-agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBz_56r109UL",
        "outputId": "b94be4a9-c1f3-4b81-95a4-046c86fce467"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ghadf\\OneDrive\\Desktop\\Data Analytics\\Python\\DL\\git\\Deep_learning\\object_detection_roboflow\\validation_notebooks\n",
            "C:\\Users\\ghadf\n",
            "Basketball-2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# the path to BasketBall-2 dataset is object_detection_roboflow\\validation_notebooks\\Basketball-2\n",
        "#what path does Path() return?\n",
        "# Path() returns the current working directory\n",
        "# Path.cwd() returns the current working directory\n",
        "# Path.home() returns the home directory\n",
        "\n",
        "print(Path.cwd())\n",
        "print(Path.home())\n",
        "print(Path('Basketball-2'))\n",
        "\n",
        "image_path = Path('Basketball-2')\n",
        "\n",
        "\n",
        "def walk_through_dir(dir_path):\n",
        "  \"\"\"\n",
        "  Walks through dir_path returning its contents.\n",
        "  Args:\n",
        "    dir_path (str or pathlib.Path): target directory\n",
        "\n",
        "  Returns:\n",
        "    A print out of:\n",
        "      number of subdiretories in dir_path\n",
        "      number of images (files) in each subdirectory\n",
        "      name of each subdirectory\n",
        "  \"\"\"\n",
        "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
        "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
        "\n",
        "\n",
        "walk_through_dir(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "0uDmzKBs0-v7",
        "outputId": "e93a1f82-7dd7-46cb-ed4e-6e0d9bde6127"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "Cannot choose from an empty sequence",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ghadf\\OneDrive\\Desktop\\Data Analytics\\Python\\DL\\git\\Deep_learning\\object_detection_roboflow\\validation_notebooks\\Object_detection_RCNN_Pytorch.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m image_path_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(image_path\u001b[39m.\u001b[39mglob(\u001b[39m\"\u001b[39m\u001b[39m*/*/*.jpg\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# 2. Get random image path\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m random_image_path \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39;49mchoice(image_path_list)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m image_class \u001b[39m=\u001b[39m random_image_path\u001b[39m.\u001b[39mparent\u001b[39m.\u001b[39mstem\n",
            "File \u001b[1;32mc:\\Users\\ghadf\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\random.py:373\u001b[0m, in \u001b[0;36mRandom.choice\u001b[1;34m(self, seq)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[39m# As an accommodation for NumPy, we don't use \"if not seq\"\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[39m# because bool(numpy.array()) raises a ValueError.\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(seq):\n\u001b[1;32m--> 373\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCannot choose from an empty sequence\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    374\u001b[0m \u001b[39mreturn\u001b[39;00m seq[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_randbelow(\u001b[39mlen\u001b[39m(seq))]\n",
            "\u001b[1;31mIndexError\u001b[0m: Cannot choose from an empty sequence"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "# Set seed\n",
        "random.seed(42) # <- try changing this and see what happens\n",
        "\n",
        "# 1. Get all image paths (* means \"any combination\")\n",
        "image_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n",
        "\n",
        "# 2. Get random image path\n",
        "random_image_path = random.choice(image_path_list)\n",
        "\n",
        "# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\n",
        "image_class = random_image_path.parent.stem\n",
        "\n",
        "# 4. Open image\n",
        "img = Image.open(random_image_path)\n",
        "\n",
        "# 5. Print metadata\n",
        "print(f\"Random image path: {random_image_path}\")\n",
        "print(f\"Image class: {image_class}\")\n",
        "print(f\"Image height: {img.height}\")\n",
        "print(f\"Image width: {img.width}\")\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj4Rn6qoCGVx"
      },
      "source": [
        "add in a bounding box checker for before and after transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SKwlvbkP1BNM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# Define a consistent transform for images\n",
        "def get_transform(train):\n",
        "    transforms_list = [transforms.ToTensor()]\n",
        "    if train:\n",
        "        transforms_list.extend([\n",
        "            transforms.Resize((224, 224), antialias=True),\n",
        "            transforms.RandomHorizontalFlip(0.5)\n",
        "        ])\n",
        "    else:\n",
        "        transforms_list.append(transforms.Resize((224, 224), antialias=True))\n",
        "    return transforms.Compose(transforms_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9DwtoR1tIYPJ"
      },
      "outputs": [],
      "source": [
        "def verify_and_correct_boxes(boxes, image_width, image_height):\n",
        "    corrected_boxes = []\n",
        "    for box in boxes:\n",
        "        # Extract coordinates\n",
        "        x_center, y_center, box_width, box_height = box\n",
        "\n",
        "        # Convert centers to top-left coordinates\n",
        "        x_min = (x_center - box_width / 2) * image_width\n",
        "        y_min = (y_center - box_height / 2) * image_height\n",
        "\n",
        "        # Ensure width and height are positive\n",
        "        box_width = abs(box_width * image_width)\n",
        "        box_height = abs(box_height * image_height)\n",
        "\n",
        "        # Convert to bottom-right coordinates\n",
        "        x_max = x_min + box_width\n",
        "        y_max = y_min + box_height\n",
        "\n",
        "        # Clip boxes that are out of image bounds\n",
        "        x_min = max(0, min(x_min, image_width - 1))\n",
        "        y_min = max(0, min(y_min, image_height - 1))\n",
        "        x_max = max(x_min + 1, min(x_max, image_width))\n",
        "        y_max = max(y_min + 1, min(y_max, image_height))\n",
        "\n",
        "        corrected_boxes.append((x_min, y_min, x_max, y_max))\n",
        "\n",
        "    return torch.tensor(corrected_boxes, dtype=torch.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "y9ptMDWT1Mjm"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, img_dir, label_dir, transform=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.transform = transform\n",
        "        self.img_names = [img_name for img_name in os.listdir(img_dir) if img_name.endswith('.jpg')]\n",
        "\n",
        "    def load_labels(self, label_path):\n",
        "        with open(label_path, 'r') as file:\n",
        "            labels = file.read().splitlines()\n",
        "\n",
        "        annotations = []\n",
        "        for label in labels:\n",
        "            elements = label.split()\n",
        "            # Only process labels with exactly 5 elements\n",
        "            if len(elements) == 5:\n",
        "                class_label, x_center, y_center, width, height = map(float, elements)\n",
        "                annotations.append((class_label, x_center, y_center, width, height))\n",
        "            else:\n",
        "                print(f\"Skipping invalid label in {label_path}: {label}\")\n",
        "        return annotations\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_names[idx])\n",
        "        label_path = os.path.join(self.label_dir, self.img_names[idx].replace('.jpg', '.txt'))\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Store original image size before transformation\n",
        "        orig_width, orig_height = image.size\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # If your transforms include resizing, then you should get the new size from the transformed image\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            # For tensors, the size is in the shape attribute\n",
        "            img_height, img_width = image.shape[-2:]\n",
        "        else:\n",
        "            # If the image is not a tensor, it's still a PIL image, and we can use the size method\n",
        "            img_width, img_height = image.size\n",
        "\n",
        "        annotations = self.load_labels(label_path)\n",
        "        boxes = [ann[1:] for ann in annotations]  # Extract only the bounding box coordinates\n",
        "        corrected_boxes = verify_and_correct_boxes(boxes, orig_width, orig_height)  # Use the original size here for correction\n",
        "\n",
        "        labels = [ann[0] for ann in annotations]  # Extract only the labels\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)  # Convert labels to tensor\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (corrected_boxes[:, 3] - corrected_boxes[:, 1]) * (corrected_boxes[:, 2] - corrected_boxes[:, 0])\n",
        "        iscrowd = torch.zeros((len(annotations),), dtype=torch.int64)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": corrected_boxes,\n",
        "            \"labels\": labels,\n",
        "            \"image_id\": image_id,\n",
        "            \"area\": area,\n",
        "            \"iscrowd\": iscrowd\n",
        "        }\n",
        "\n",
        "        return image, target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KeL_Wf-E1YoH"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[WinError 3] The system cannot find the path specified: 'Basketball-2/train/images'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ghadf\\OneDrive\\Desktop\\Data Analytics\\Python\\DL\\git\\Deep_learning\\object_detection_roboflow\\validation_notebooks\\Object_detection_RCNN_Pytorch.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Set up the dataset and dataloader\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_data \u001b[39m=\u001b[39m CustomDataset(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     img_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mBasketball-2/train/images\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     label_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mBasketball-2/train/labels\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     transform\u001b[39m=\u001b[39;49mget_transform(train\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m test_data \u001b[39m=\u001b[39m CustomDataset(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     img_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBasketball-2/test/images\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     label_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBasketball-2/test/labels\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     transform\u001b[39m=\u001b[39mget_transform(train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\n",
            "\u001b[1;32mc:\\Users\\ghadf\\OneDrive\\Desktop\\Data Analytics\\Python\\DL\\git\\Deep_learning\\object_detection_roboflow\\validation_notebooks\\Object_detection_RCNN_Pytorch.ipynb Cell 10\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_dir \u001b[39m=\u001b[39m label_dir\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39m=\u001b[39m transform\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_names \u001b[39m=\u001b[39m [img_name \u001b[39mfor\u001b[39;00m img_name \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(img_dir) \u001b[39mif\u001b[39;00m img_name\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.jpg\u001b[39m\u001b[39m'\u001b[39m)]\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'Basketball-2/train/images'"
          ]
        }
      ],
      "source": [
        "# Set up the dataset and dataloader\n",
        "train_data = CustomDataset(\n",
        "    img_dir='Basketball-2/train/images',\n",
        "    label_dir='Basketball-2/train/labels',\n",
        "    transform=get_transform(train=True)\n",
        ")\n",
        "\n",
        "test_data = CustomDataset(\n",
        "    img_dir='Basketball-2/test/images',\n",
        "    label_dir='Basketball-2/test/labels',\n",
        "    transform=get_transform(train=False)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bl0-YeSp1afC"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_data, batch_size=2, shuffle=True, collate_fn=collate_fn, num_workers=2\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    test_data, batch_size=2, shuffle=False, collate_fn=collate_fn, num_workers=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRjPaHnbDrAz",
        "outputId": "aedf91c6-f55c-4111-8be7-4963bb532ff8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0-3): 4 x Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=6, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=24, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Model setup\n",
        "num_classes = 6  # 5 classes + background\n",
        "\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jkzXmbw6PfWb"
      },
      "outputs": [],
      "source": [
        "from torchvision.ops import box_iou\n",
        "\n",
        " #New IoU based metrics calculation function\n",
        "def calculate_iou_metrics(pred_boxes, true_boxes, iou_threshold=0.5):\n",
        "    true_positives = 0\n",
        "    false_positives = 0\n",
        "    false_negatives = 0\n",
        "\n",
        "    for pred, true in zip(pred_boxes, true_boxes):\n",
        "        # Check if either predicted or true boxes are empty\n",
        "        if pred['boxes'].nelement() == 0 or true['boxes'].nelement() == 0:\n",
        "            if pred['boxes'].nelement() == 0 and true['boxes'].nelement() != 0:\n",
        "                false_negatives += len(true['boxes'])\n",
        "            continue\n",
        "\n",
        "        iou = box_iou(pred['boxes'], true['boxes'])\n",
        "\n",
        "        for i in range(len(pred['boxes'])):\n",
        "            if iou[i].max() > iou_threshold:\n",
        "                true_positives += 1\n",
        "            else:\n",
        "                false_positives += 1\n",
        "\n",
        "        false_negatives += len(true['boxes']) - iou.max(dim=0)[0].sum().item()\n",
        "\n",
        "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
        "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
        "    return precision, recall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "i48KAumQG9U_"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(pred_boxes, true_boxes, iou_threshold=0.5):\n",
        "    accuracy = 0.0\n",
        "    for pred, true in zip(pred_boxes, true_boxes):\n",
        "        iou = box_iou(pred['boxes'], true['boxes'])\n",
        "        correct_predictions = iou > iou_threshold\n",
        "\n",
        "        # Check for division by zero\n",
        "        if correct_predictions.numel() == 0:\n",
        "            continue\n",
        "\n",
        "        accuracy += correct_predictions.sum().item() / correct_predictions.numel()\n",
        "    return accuracy / len(pred_boxes) if len(pred_boxes) > 0 else 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mskdsMLkTnaV"
      },
      "outputs": [],
      "source": [
        "# Training function remains the same\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_accuracy = 0.0\n",
        "    for batch, (images, targets) in enumerate(data_loader):\n",
        "        images = list(img.to(device) for img in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        if not torch.isfinite(losses):\n",
        "            print(\"Loss is not finite, stopping training.\")\n",
        "            return None\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += losses.item()\n",
        "        train_accuracy += calculate_accuracy(targets, targets)\n",
        "\n",
        "    train_loss /= len(data_loader)\n",
        "    train_accuracy /= len(data_loader)\n",
        "\n",
        "    print(f\"train_loss: {train_loss}, train_accuracy: {train_accuracy}\")\n",
        "    return train_loss, train_accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5mfErFkvTo32"
      },
      "outputs": [],
      "source": [
        "# Modified evaluate function\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    test_precision = 0.0\n",
        "    test_recall = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = list(img.to(device) for img in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "            preds = model(images)\n",
        "\n",
        "            precision, recall = calculate_iou_metrics(preds, targets)\n",
        "            test_precision += precision\n",
        "            test_recall += recall\n",
        "\n",
        "    test_precision /= len(data_loader)\n",
        "    test_recall /= len(data_loader)\n",
        "\n",
        "    print(f\"test_precision: {test_precision}, test_recall: {test_recall}\")\n",
        "    return test_precision, test_recall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cJFE-96m4BC6"
      },
      "outputs": [],
      "source": [
        "# Adjusted train and evaluate loop\n",
        "def train_and_evaluate(model, train_dataloader, test_dataloader, optimizer, lr_scheduler, device, num_epochs):\n",
        "    train_losses = []\n",
        "    test_iou_scores = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_accuracy = train_one_epoch(model, optimizer, train_dataloader, device, epoch)\n",
        "        if train_loss is None:\n",
        "            print(\"Training stopped due to an error.\")\n",
        "            break\n",
        "\n",
        "        test_precision, test_recall = evaluate(model, test_dataloader, device)\n",
        "        if test_precision is None or test_recall is None:\n",
        "            print(\"Evaluation stopped due to an error.\")\n",
        "            break\n",
        "\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        test_iou_scores.append((test_precision, test_recall))\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train loss: {train_loss}, Train accuracy: {train_accuracy}, Test precision: {test_precision}, Test recall: {test_recall}\")\n",
        "\n",
        "    return train_losses, test_iou_scores\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Fvwgu9UDTvjM"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ghadf\\OneDrive\\Desktop\\Data Analytics\\Python\\DL\\git\\Deep_learning\\object_detection_roboflow\\validation_notebooks\\Object_detection_RCNN_Pytorch.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Simplified training loop setup\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m, weight_decay\u001b[39m=\u001b[39m\u001b[39m0.0005\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m lr_scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mStepLR(optimizer, step_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#num_epochs = 10\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Train the model and get metrics\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# Simplified training loop setup\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "num_epochs = 10\n",
        "\n",
        "# Train the model and get metrics\n",
        "train_losses, test_iou_scores = train_and_evaluate(model, train_dataloader, test_dataloader, optimizer, lr_scheduler, device, num_epochs)\n",
        "\n",
        "\n",
        "# Call the train_and_evaluate function\n",
        "#train_and_evaluate(model, train_dataloader, test_dataloader, optimizer, lr_scheduler, device, num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGTsgKC9a4kf"
      },
      "source": [
        "Looking at training and test loss curves is a great way to see if your model is overfitting.\n",
        "\n",
        "An overfitting model is one that performs better (often by a considerable margin) on the training set than the validation/test set.\n",
        "\n",
        "If your training loss is far lower than your test loss, your model is overfitting.\n",
        "\n",
        "As in, it's learning the patterns in the training too well and those patterns aren't generalizing to the test data.\n",
        "\n",
        "The other side is when your training and test loss are not as low as you'd like, this is considered underfitting.\n",
        "\n",
        "The ideal position for a training and test loss curve is for them to line up closely with each other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqKFQYRea4Ha"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the training loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.title('Training Loss Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting the testing IoU\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(test_iou_scores, label='Testing IoU Score', color='orange')\n",
        "plt.title('Testing IoU Over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('IoU Score')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ghadf\\OneDrive\\Desktop\\Data Analytics\\Python\\DL\\git\\Deep_learning\\object_detection_roboflow\\validation_notebooks\\Object_detection_RCNN_Pytorch.ipynb Cell 21\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#load to infer on new images\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mmodel_state_dict.pth\u001b[39;49m\u001b[39m'\u001b[39;49m), map_location\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ghadf/OneDrive/Desktop/Data%20Analytics/Python/DL/git/Deep_learning/object_detection_roboflow/validation_notebooks/Object_detection_RCNN_Pytorch.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#model.to(device)\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\ghadf\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[0;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[0;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\ghadf\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[1;32mc:\\Users\\ghadf\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1141\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1142\u001b[0m     typed_storage \u001b[39m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[0;32m   1144\u001b[0m \u001b[39mreturn\u001b[39;00m typed_storage\n",
            "File \u001b[1;32mc:\\Users\\ghadf\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:1116\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1112\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39mUntypedStorage)\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_untyped_storage\n\u001b[0;32m   1113\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1114\u001b[0m \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1116\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[0;32m   1117\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m   1118\u001b[0m     _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m typed_storage\u001b[39m.\u001b[39m_data_ptr() \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1121\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m typed_storage\n",
            "File \u001b[1;32mc:\\Users\\ghadf\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 217\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[0;32m    218\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[1;32mc:\\Users\\ghadf\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:182\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[0;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m--> 182\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[0;32m    183\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    184\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
            "File \u001b[1;32mc:\\Users\\ghadf\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\serialization.py:166\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    163\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    167\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    168\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    169\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    170\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    171\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "#load to infer on new images\n",
        "model.load_state_dict(torch.load('model_state_dict.pth'), map_location=torch.device('cpu'))\n",
        "model.eval()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import os\n",
        "\n",
        "# Replace this with the direct URL of the image you want to download\n",
        "image_url = \"https://www.google.com/search?client=firefox-b-1-d&sca_esv=3327f1c9d2227784&sxsrf=AM9HkKlt52SlfwUmlCipyy1q9dCgaGxKZA:1701532475460&q=stephen+curry+shooting+threes&tbm=isch&source=lnms&sa=X&ved=2ahUKEwiA5Ze2jvGCAxWcVTABHZunDX8Q0pQJegQIDBAB&biw=1600&bih=739&dpr=1#imgrc=qPWrl1aHYQs17M\"\n",
        "image_url = \"https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.sbnation.com%2Fnba%2F2017%2F3%2F6%2F14823068%2F2017-nba-scores-steph-curry-gobert-ulis-westbrook&psig=AOvVaw3BAtfjqcv949FdTA2tM5yb&ust=1701618880377000&source=images&cd=vfe&opi=89978449&ved=0CBIQjRxqFwoTCMCQwbqO8YIDFQAAAAAdAAAAABAE\"\n",
        "image_url = \"https://cdn.vox-cdn.com/thumbor/nPF0yG3Mi4c09o4MfRag0bLi8_g=/0x0:4497x2998/920x613/filters:focal(1280x131:1998x849):format(webp)/cdn.vox-cdn.com/uploads/chorus_image/image/53550403/usa_today_9920467.0.jpg\"\n",
        "\n",
        "\n",
        "# Fetch the image from the URL\n",
        "response = requests.get(image_url)\n",
        "if response.status_code == 200:\n",
        "    image = Image.open(BytesIO(response.content))\n",
        "\n",
        "    # Path to save the image in Colab's local storage\n",
        "    folder_path = '/content/inference_images/'  # Local directory in Google Colab\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "    # Save the image\n",
        "    file_path = os.path.join(folder_path, 'stephen_curry_shooting.jpeg')\n",
        "    image.save(file_path)\n",
        "\n",
        "    print(f\"Image saved at {file_path}\")\n",
        "else:\n",
        "    print(f\"Failed to download the image. Status code: {response.status_code}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Define the transform\n",
        "def get_transform():\n",
        "    return transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((224, 224)),  # Resize if necessary\n",
        "    ])\n",
        "\n",
        "# Load an image\n",
        "image_path = '/content/inference_images/stephen_curry_shooting.jpeg'\n",
        "img = Image.open(image_path).convert('RGB')\n",
        "transformed_img = get_transform()(img).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    prediction = model([transformed_img])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Function to display image with bounding boxes\n",
        "def display_image_with_boxes(image, boxes, labels, scores, threshold=0.5):\n",
        "    # Convert image tensor to numpy\n",
        "    image = image.cpu().numpy().transpose(1, 2, 0)\n",
        "    fig, ax = plt.subplots(1, figsize=(12, 9))\n",
        "    ax.imshow(image)\n",
        "\n",
        "    for box, label, score in zip(boxes, labels, scores):\n",
        "        if score > threshold:\n",
        "            x, y, x2, y2 = box\n",
        "            bbox = patches.Rectangle((x, y), x2 - x, y2 - y, linewidth=2, edgecolor='r', facecolor='none')\n",
        "            ax.add_patch(bbox)\n",
        "            plt.text(x, y, f'Class: {label}, Score: {score:.2f}', bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Process the prediction\n",
        "pred_boxes = [box.cpu().numpy() for box in prediction[0]['boxes']]\n",
        "pred_labels = [label.cpu().numpy() for label in prediction[0]['labels']]\n",
        "pred_scores = [score.cpu().numpy() for score in prediction[0]['scores']]\n",
        "\n",
        "# Display the image\n",
        "display_image_with_boxes(transformed_img, pred_boxes, pred_labels, pred_scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
